\chapter{Data Model}
\label{ch:datamodel}
\newpage

\section{Introduction and Description of the Data}
\label{sec:description_pvdata}
A data model was designed to show the energy production of the photovoltaic system. The model was based on real measured data, comprising 10,968 hourly measurements of the production of a 48 kWp PV system, as well as hourly averages of temperature, humidity and global radiation, collected between July 2018 and December 2019. In the model used for the use-case, an ensemble learning variant of decision tree induction was applied. This random forest regression can map the influence of the weather values on the production extremely accurately, without computationally intensive equations. In experiments on randomized training and test data in a ratio of 80 to 20, the model achieved determination coefficients of 0.96 on the training data and 0.87 on the reference data. The model is therefore sufficiently generalizable. The methods for preparing the data and implementing the model are based on the framework developed by Schranz et al. 2020 \cite{schranz2020water}.

\section{Machine learning}

The method mentioned above which was used for the data model belongs to the machine learning tools. Machine learning is a subfield of computer science that is concerned with building algorithms which, to be useful, rely on a collection of examples of some phenomenon. These examples can come from nature, be handcrafted by humans or generated by another algorithm. \\
Machine learning can also be defined as the process of solving a practical problem by 1) gathering a dataset, and 2) algorithmically building a statistical model based on that dataset. \\
Learning can be supervised, semi-supervised, unsupervised and reinforcement.In the following sections, several supervised learning methods which were applied to the same PV datasetwill be presented. This includes a brief discussion of the method and a comparison of the results. \\
\\


\subsection{Supervised learning}

In supervised learning, the dataset is the collection of labeled examples. Each element x$_{i}$ among N is called a feature vector. A feature vector is a vector in which each dimension j = 1, . . . ,D contains a value that describes the example somehow. That value is called a feature and is denoted as x(j). In our case, we have three feature vectors, the temerature, the global irridation and the humidity. For all examples in the dataset, the feature at position j in the feature vector always contains the same kind of information. In our example this information is the generated power by the photovoltaic system. \\
\\

The goal of a supervised learning algorithm is to use the dataset to produce a model that takes a feature vector x as input and outputs information that allows deducing the label for this feature vector. For instance, the model created using the dataset of people could take as input a feature vector describing a person and output a probability that the person has cancer. \cite{thehundrerpageMLbook}

\subsection*{Example Spam Filter}
\label{subsec:SVM_Spam}

The supervised learning process starts with gathering the data. The data for supervised learning is a collection of pairs (input, output). Input could be anything, for example, email messages, pictures, or sensor %measurements. Outputs are usually real numbers, or labels such as “spam”, “not_spam”, “cat”, “dog”. \\
Let us take the example span detection to give an step by step introduction to classification by supervised learning. The first step is to gather the data, for example 10,000 email messages, each with a label %“spam” or “not_spam”. \\
The data analyst decides, based on experience, how to convert a real-world entity, such as an email message, into a feature vector. One common way to convert a text into a feature vector, called bag of words, is to take a dictionary of English words. If one assumes a dictionary that contains 20,000 alphabetically sorted words, the conversion into a feature vector works as follows: 


\begin{description}
  \item[$\bullet$] the first feature is equal to 1 if the email message contains the word “a”; otherwise,this feature is 0
  \item[$\bullet$] the second feature is equal to 1 if the email message contains the word “aaron”; otherwise,this feature equals 0;
  \item[$\bullet$] ...
  \item[$\bullet$] the feature at position 20,000 is equal to 1 if the email message contains the word “zulu”; otherwise, this feature is equal to 0.
 
One repeats the above procedure for every email message in the collection, which gives 10,000 feature vectors, each vector having the dimensionality %of 20,000 and a label “spam” / “not_spam”.
\\

Now one has a machine-readable input data, but the output labels are still in the form of human-readable text. Some learning algorithms require transforming labels into numbers. For example, some algorithms require numbers like 0 (to represent the label %“not_spam”)
and 1 %(to represent the label “spam”). 
The algorithm used here to illustrate supervised learning is called Support Vector Machine (SVM). This algorithm requires that the positive label has the numeric value (spam) of  +1 (plus one), and the negative label (notspam) has the value of -1 (minus one). \\
At this point, one has a dataset and a learning algorithm, now the learning algorithm can be applied to the dataset to get the model. \\
\\
SVM sees every feature vector as a point in a high-dimensional space. In this example, the space has 20,000 dimensions, since the feature vector has 20,000 entries. The algorithm puts all feature vectors on an imaginary 20,000-dimensional plot and draws an imaginary 19,999-dimensional line (a hyperplane) that separates examples with positive labels from examples with negative labels.\\
The equation of the hyperplane is given by two parameters, a real-valued vector w of the same dimensionality as our input feature vector x, and a real number b:

\begin{equation}
	\textbf{w} \cdot \textbf{x} - b = 0
\end{equation}

The goal of the learning algorithm — SVM in this case — is to leverage the dataset and find the optimal values w* and b* for parameters w and b. Once the learning algorithm identifies these optimal values, the model f(x) is then defined as:

\begin{equation}
	f(x) = sign(\textbf{w}^{*} \cdot \textbf{x}^{*} - b)
\end{equation}

Therefore, to predict whether an email message is spam or not spam using an SVM model, you have to take a text of the message, convert it into a feature vector, then multiply this vector by w\*, subtract b\* and take the sign of the result. 
This will give us the prediction, plus 1 means spam, minus 1 means not-spam. \\
For two-dimensional feature vectors, the problem and the solution can be visualized as shown in Figure \ref{fig:example_svm}. The blue and orange circles represent, respectively, positive and negative
examples, and the line given by $\textbf{w} \cdot \textbf{x} - b = 0$ is the decision boundary.
\end{description}

In practice, there are two other essential differentiators of learning algorithms to consider: speed of model building and prediction processing time. In many practical cases, you would prefer a learning algorithm that builds a less accurate model fast. Additionally, you might prefer a less accurate model that is much quicker at making predictions.


\myfig{example_svm}%% filename in figures folder
  {width=0.5\textwidth,height=0.5\textheight}%% maximum width/height, aspect ratio will be kept
  {An example of an SVM model for two-dimensional feature vectors.}%% caption
  {}%% optional (short) caption for table of figures
  {fig:example_svm}%% label  
  
  


\cite{thehundrerpageMLbook}



\section{Different Fields of Application}

\subsection{Classification}

Classification is a problem of automatically assigning a label to an unlabeled example. Spam detection is a famous example of classification, or to classify a picture in whether it shows a cat or a dog. \\
In machine learning, the classification problem is solved by a classification learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and either directly output a label or output a number that can be used by the analyst to deduce the label. This number could be a probability for example. \\
\\
In a classification problem, a label is a member of a finite set of classes. If the size of the set of classes is two, we talk about binary classification. Multiclass classification is a classification problem with three or more classes, to decide which number from zero to nine a picture shows would be an example for a multiclass classification problem.  \cite{thehundrerpageMLbook}


\subsection{Regression}  

Regression is a problem of predicting a real-valued label (often called a target) given an unlabeled example. Estimating house price valuation based on house features, such as area, the number of bedrooms, location and so on is a famous example of regression. \\
The regression problem is solved by a regression learning algorithm that takes a collection of labeled examples as inputs and produces a model that can take an unlabeled example as input and output a target.


\subsection{Clustering}
It is basically a type of unsupervised learning method . An unsupervised learning method is a method in which we draw references from datasets consisting of input data without labelled responses. Generally, it is used as a process to find meaningful structure, explanatory underlying processes, generative features, and groupings inherent in a set of examples.
Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group and dissimilar to the data points in other groups. It is basically a collection of objects on the basis of similarity and dissimilarity between them. 
%https://www.geeksforgeeks.org/clustering-in-machine-learning/
\cite{thehundrerpageMLbook}

\section{Applied Machine Learning Methods}

In the following section, the machine learning methods which were applied to the dataset of the PV system are explained in theory and the results of the simulation are given. Finally the results of the different methods are compared in Section \ref{sec:ComparisonofML}.  \\
The different machine learning algorithms were applied to the PV data described in Section \ref{sec:description_pvdata}. A train-test-split was done, that means randomly a part of the data was chosen for training the model and another part for testing the model. 80\% of the data was used to train the model, the remaining 20\% of the data was used to test the model. To measure the quality of the model, the $R^{2}$ value was calculated, for the training data and - with a higher significance - also for the test data. The test data was never seen by the model during the training process, so it is new data and one can check the quality of the prediction with it.


\subsection{Linear Regression}

Linear regression is a popular regression learning algorithm that learns a model which is a linear combination of features of the input example. \\
One wants to buid a model $f_{\textbf{w},b}(x)$ as a linear combination of features of example x: 

\begin{equation}
\label{eq:linregression}
	f_{\textbf{w},b}(x) = \textbf{w} x + b
\end{equation}


An example for a linear regression is given in Figure \ref{lin_regression}. One can notice the similarity to the SVM which is explained in Section \ref{subsec:SVM_Spam}, especially Figure \ref{example_svm} has similar aspects as the linear regression.  The two models are similar, but the hyperplane in the SVM plays the role of the decision boundary, it is used to separate two groups of examples from one another, while in the linear regression we want to fit our data points as good as possible. \\
To do so, one needs a objective function which is minimized in the optimization process. The objective function, or loss function, is called the squared error loss. This is an arbitrary decision, one could also use the absolute value of the difference or the cube instead of a square. \\
The loss function is defined by: 

\begin{equation}
   f_{loss} = \sum_{i = 1}^{N} (f_{\textbf{w},b}(x_{i}) - y_i)^{2} 
\end{equation}

In the optimization procedure, the optimal parameters $\textbf{w}^{*}$ and $b^{*}$ of equation \ref{eq:linregression} are found to minimize the loss function. 

\myfig{lin_regression}%% filename in figures folder
  {width=0.5\textwidth,height=0.5\textheight}%% maximum width/height, aspect ratio will be kept
  {An example of an SVM model for two-dimensional feature vectors.}%% caption
  {}%% optional (short) caption for table of figures
  {fig:lin_regression}%% label  
  
 
  
\begin{table}%[!h]
\centering
\caption{Results of the linear regression algorithm \label{tab:lin_regression}}
\vspace{1mm}
\begin{tabular}
{| c | c | } 
\hline
\textbf{Methode} & Linear Regression \\ \hline
R$^{2}$(train) & $0.83$ \\ \hline
$R^{2}$(test) & $0.83$ \\ \hline 
time & 5 ms \\ \hline

\end{tabular}
\end{table}
  
  
\subsection{Polynomial Regression}

The polynomial regression works similar as the linear regression, but polynomials of higher orders are used. In this example, a hyperparameter fitting was used to figure out the best degree of the polynomial. The result shows that a polynomial of degree 5 gives the best results. \\

\begin{table}%[!h]
\centering
\caption{Results of the polynomial regression algorithm and comparison of the degrees \label{tab:pol_regression}}
\vspace{1mm}
\begin{tabular}
{| c | c | } 
\hline
\textbf{Methode} & Polynomial Regression (Degree = 2) \\ \hline
$R^{2}$(train) & $0.85$ \\ \hline
$R^{2}$(test) & $0.86$ \\ \hline 
time & 5 ms \\ \hline
\textbf{Methode} & Polynomial Regression (Degree = 5) \\ \hline
$R^{2}$(train) & $0.86$ \\ \hline
$R^{2}$(test) & $0.87$ \\ \hline 
time & 5 ms \\ \hline

\end{tabular}
\end{table}
  
As can be seen in Table \ref{tab:pol_regression}, the polynomial with the degree 5 gives the best results on the training and the test data. Nevertheless, since the two models show a similar precision and the difference is smaller than 1\%, the model with the lower degree is favored. This is because an effeced called overfitting. The model adapts too strongly to the provided data. This happens with polynomials of higher degrees, so we prefer the one with the lower degree to omit this effect.  


\myfig{under_overfit}%% filename in figures folder
  {width=0.5\textwidth,height=0.5\textheight}%% maximum width/height, aspect ratio will be kept
  {Examples ov underfitting, good fitting and overfitting on the same data. The underfitting curve (degree = 1) is too generalized, the quadratic function (degree = 2) shows a good fitting behaviour, the overfitting curve (degree = 15) is strongly overfitted. It gives good result only on this data, on new data it will no work. }%% caption
  {}%% optional (short) caption for table of figures
  {fig:overfitting}%% label 
  
In Figure \ref{fig:overfitting} one can see the effect of overfitting to the data due to the use of a high degree polynomial. In this case a polynomial of degree 2 has a good fitting behaviour. The overfitting curve does give good results on the training data, but if one tests it with new data it will not return good results. 
  
\subsection{Random Tree Regression}

\subsection{Random Forest Regression}

\section{Neural Network}

An artificial neural network (NN) consists of \enquote{artificial neurons}, which are arranged in layers. Each neuron of a layer is connected to all neurons of the directly adjacent layer. So an NN takes information about its input layer and outputs the result of its calculation at the output layer. In between there are several hidden layers, where the main part of the information processing happens. \\
\\
This structure can be seen in Figure \ref{fig:NN_structure}. An neural network is made up of layers. In this Figure, on the left one can see the input neurons. This is where information such as a recognizable image or text is entered. A representation of the information within the states of the individual input neurons (A, B and C) is selected. Each new piece of information that is to be entered into the NN must be represented in the input layer according to the same scheme. The input neurons are linked to the neurons of the next layer via weighted connections. The signal of the input neurons is therefore passed on to the following neurons with different degrees of intensity and processed there. When the signals of all inputs of a neuron have been processed, the resulting signal is again forwarded in the same way as before. Finally, the result of the NN can be read at the output layer. Should this neuron light up and have a strong output signal, this can be interpreted in terms of its significance as a result. \cite{ZesarBewusstsein}

\myfig{NN_structure}%% filename in figures folder
  {width=0.6\textwidth,height=0.6\textheight}%% maximum width/height, aspect ratio will be kept
  {The structure of a neural network. It is constructed by different types of layers. \cite{ZesarBewusstsein} }%% caption
  {}%% optional (short) caption for table of figures
  {fig:NN_structure}%% label 
  
  

Various activation functions of neurons in NN can be seen in Figure \ref{fig:NN_signals}. \textbf{A} The step function. The all-or-nothing reaction of biological neurons (step function - continuous line) occurs when a defined threshold is reached. Below this threshold there is no reaction. Artificial neurons use the Sigmoid function (dashed line), which is a blurred version of the step function can be understood. A small change in the input signal therefore requires always a small change in the output signal. \textbf{B} The hyperbolic tangent function is often used in neural networks and is similar to the sigmoid function, but symmetrical around the zero point. Here negative output signals are possible. \textbf{C} A rectification function as it is used in ReLUs. Positive signals are passed on, negative do not provoke any reaction. ReLUs are particularly common in low NN and are particularly ecient. \textbf{D} A further development of the ReLUs is the PReLU function. This unit also allows negative output signals, but weakens them off. The degree of attenuation in the negative range is determined during the training of the NN. 
\cite{ZesarBewusstsein}

\myfig{NN_signals}%% filename in figures folder
  {width=0.6\textwidth,height=0.6\textheight}%% maximum width/height, aspect ratio will be kept
  {The structure of a neural network. It is constructed by different types of layers. \cite{ZesarBewusstsein} }%% caption
  {}%% optional (short) caption for table of figures
  {fig:NN_signals}%% label   
  
In Figure \ref{fig:NN_numbers}, one can see a NN for the recognition of handwritten numbers. The used dataset is from hte MNIST data bank and consists of 60000 pictures for the training of the NN and 10000 pictures for the testing. Examples for handwritten number can be seen in Figure \ref{fig:mnist}. \\
Each picture consists of $28 x 28$ pixels, these are transformed to a vector with 784 entries, or 784 dimensions. The information of each pixel is a gray scale value. This information is the input layer, so one needs accordingly 784 input neurons, one for each pixel. The gray scale value of each pixel is assigned to one input neuron and is represented by a value between 0 and 1, where 1 is black and 0 is white. \\
The state of the neurons in the input layer is forwarded via their connections. On the way trough the connection, the state E (number between 0 and 1) is multiplied by the weight  w of the respective connection (number between 1 and 1). In the neurons of the hidden layer all weighted states are summed up to an Eges. An output signal is calculated from the Eges via the activation function of the respective neuron. This signal is then forwarded to numerous connections and reweighted. At the output layer, the incoming signals are also summed. That neuron within the output layer, which reaches the highest numerical value after summation, represents the result.
%bilderkennungsprozess beschreiben, udemy kurs ansehen
  
    \myfig{NN_mnist}%% filename in figures folder
  {width=0.6\textwidth,height=0.6\textheight}%% maximum width/height, aspect ratio will be kept
  {The structure of a neural network. It is constructed by different types of layers. \cite{ZesarBewusstsein} }%% caption
  {}%% optional (short) caption for table of figures
  {fig:NN_mnist}%% label
  
  
  
  \myfig{NN_numbers}%% filename in figures folder
  {width=0.6\textwidth,height=0.6\textheight}%% maximum width/height, aspect ratio will be kept
  {The Neural Network of a recognition software for handwritten numbers. In this example, the neuron associated with the number 3 has the highest value and represents the result correctly.}%% caption
  {}%% optional (short) caption for table of figures
  {fig:NN_numbers}%% label 
  
\section{Comparison of the results}


